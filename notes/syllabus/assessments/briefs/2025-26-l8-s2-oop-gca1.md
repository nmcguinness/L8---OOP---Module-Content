| **Item**                | **Details**                                                                                                                                                                                     |
| :---------------------- | :---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| **Programme**           | BSc (Hons) in Computing in Games Development; BSc (Hons) in Computing in Software Development                                                                                                   |
| **Stage**               | 2                                                                                                                                                                                               |
| **Module**              | COMP C8Z03 (Object-Oriented Programming)                                                                                                                                                        |
| **Weight**              | 20% of module grade                                                                                                                                                                             |
| **Submission**          | GitHub URL via Moodle (one per pair). Public repo required.                                                                                                                                     |
| **Oral Defense**        | Students will normally demonstrate their project and defend it in a brief interview. Your demonstrated understanding of the work presented is a major factor in grading.                        |
| **Late Submission**     | Institute policy on late submission will apply (see [here](https://www.dkit.ie/about/policies/continuous-assessment-procedures)).                                                               |
| **Academic Integrity**  | Institute policy on academic integrity will apply (see [here](https://www.dkit.ie/about/policies/academic-integrity-policy-and-procedures)).                                                    |
| **Generative AI Tools** | Institute policy on the use of Generative AI tools will apply (see [here](https://www.dkit.ie/about/policies/generative-artificial-intelligence-ai-and-your-assessments-a-guide-for-students)). |
| **CA Cover Sheet**      | A signed CA cover sheet must be included and may be found (see [here](https://www.dkit.ie/about/policies/continuous-assessment-procedures)).                                                    |



## 1) Overview
Build a small **Java data system** (e.g., leaderboard, products, customers, bookings, events) in **pairs** across **two stages**:
- Model a realistic entity with validated fields.
- Load and query records using Java Collections.
- Implement ordering (Comparable/Comparator) and, later, equality & hashing.
- Practise defensive coding and DRY.
- Write a concise report and defend your work in interview.

> You may propose your own domain/dataset by **Week 6** for approval.

---

## 2) Learning outcomes assessed
- **MLO1:** Solve intermediate-to-advanced problems using OOP and console/GUI I‑P‑O.  
- **MLO2:** Use OOP concepts, models, patterns, tools, and techniques to build modular solutions.  
- **MLO3:** Choose appropriate Collections and algorithms and implement them.  
- **MLO5:** Use code-management, testing, and debugging techniques.

---

## 3) Timeline and weighting
- **Stage 1 — Design & Prototype (Week 9, 40%)**
  - Define your entity and build a working prototype using Lists.
  - Justify key design choices in a short README.

- **Stage 2 — Implementation & Verification (Week 12, 60%)**
  - Extend code with equality, hashing, testing, and larger data.
  - Verify with JUnit and present findings. Includes **demo + interview**.

**Repo structure**
- Confirm pairs by **Monday of Week 6**.  
- Branches: `stage1` and `stage2` (create `stage2` after Stage 1 feedback).

---

## 4) Data & validation (requirements for both stages)

| Requirement | Details |
|:--|:--|
| CSV files | `sample_10.csv` (10 rows with edge cases) and `dataset_1000.csv` (1000 rows, UTF‑8, comma). |
| Entity (fields) | **≥7 distinct fields:** 2 × `String`, 1 × `int`, 1 × `double`, 1 × `boolean`, 1 × `LocalDate`, 1 × `LocalDateTime`. |
| Date formats | Use ISO‑8601 in CSV (e.g., `2025-10-17`, `2025-10-17T14:05:00`). |
| Encapsulation | Private fields; validate in setters/constructors (trim, blank/range checks). |
| Invalid data | Detect, **log**, and **skip** bad rows (do not crash on expected errors). |
| Output | Provide a concise printed summary and/or CSV export as required per stage. |

---

## 5) Stage 1 — What to deliver (40%)

### Marking: Functional (24%)

| Criterion | Description | Weight |
|:--|:--|:--:|
| Entity Definition | ≥7 typed fields; encapsulated; validation with clear messages. | 6% |
| CSV Loading | Load exactly 10 rows; print valid-count and a sample listing. | 5% |
| Data Structure Choice | Use `ArrayList` or `LinkedList` with justification; show iteration and safe removal. | 3% |
| Ordering & Searching | Implement `Comparable` (natural order) and one `Comparator`; show search/sort. | 4% |
| DRY & Defensive Coding | Early exits, helper methods for repeated logic; no duplication hotspots. | 3% |
| Reporting & Output | Brief, readable console output or formatted listing. | 3% |

### Marking: Process & Quality (16%)

| Criterion | Description | Weight |
|:--|:--|:--:|
| Repository Quality | Regular, meaningful commits; branches/tags; balanced contributions. | 4% |
| Unit Testing & Coverage (baseline) | Driver/JUnit for ordering/search; **≥35%** coverage; include evidence. | 4% |
| Reporting & Documentation | Concise README/report: design rationale, defensive examples, contribution matrix, reflection. | 8% |

**Reporting guidance (prose, not including code/figures)**

| Section | Who writes it | Word count |
|:--|:--:|:--:|
| Solution Domain | both | 120–250 |
| Design Justification | both | 200–350 |
| Defensive Coding 1 (4–6 examples) | both | 220–400 total |
| Reflection | each | 100–150 |
| Commit Contributions | both | 50–120 |

---

## 6) Stage 2 — What to deliver (60%)

### Marking: Functional (36%)

| Criterion | Description | Weight |
|:--|:--|:--:|
| Extended Dataset | Load exactly 1000 rows; efficient, robust loader with error handling. | 5% |
| Entity Enhancements | Validated additions/helpers; preserve encapsulation. | 6% |
| Collections & Lookup | Introduce `HashSet`/`HashMap` for duplicates or fast lookup; show usage. | 5% |
| Equality & Hashing | Consistent `equals`/`hashCode` on stable identifiers; demonstrate effects. | 6% |
| Advanced Queries & CSV Export | 2+ queries (date range, top‑N, frequencies); summaries + CSV export (same schema). | 6% |
| Testing Demonstration | JUnit 5 tests for sorting/search/duplicates; clear assertions and naming. | 5% |
| Defensive & Reusable Code | Consistent checks and reuse; avoid repetition. | 3% |

### Marking: Process & Quality (24%)

| Criterion | Description | Weight |
|:--|:--|:--:|
| Repository Quality | Balanced activity; clear messages; branches/tags; PRs/reviews if used. | 6% |
| Unit Testing & Coverage | **≥75%** coverage target (Jacoco evidence); include positive, negative, edge cases. | 6% |
| Report + Presentation/Interview | Professional report; **live demo + interview**; clear results, rationale, and reflection. | 12% |

**Reporting guidance (prose, not including code/figures)**

| Section | Who writes it | Word count |
|:--|:--:|:--:|
| Equality & Hashing Rationale | both | 180–300 |
| Testing Summary | both | 180–300 |
| Defensive Coding 2 (4–6 examples) | both | 240–420 total |
| Reflections | both | 150–250 |
| Commit Contributions | both | 60–120 |
| Final References | both | Harvard style |

**Interview mapping**  
- Your **interview performance** may influence multiple criteria where understanding must be evidenced.  
- If the oral defense reveals a lack of understanding of submitted code, this will be reflected in the **Stage 1 (retrospectively) and Stage 2** grades in line with academic integrity and module policy.

---

## 7) Practicalities & policies

| Item | Rule |
|:--|:--|
| GitHub | **Mandatory** public repo; submit URL on Moodle. Missing repo ⇒ **not graded**. |
| Effort/Implementation Matrix | **Mandatory** in report/README. Missing ⇒ **non‑submission**. |
| Generative AI | Permitted for research/boilerplate; **declare usage** in README. Undeclared use ⇒ plagiarism. |
| References | Harvard style. |
| Late submissions | Institute procedures apply. |
| Cover Sheet | One signed CA cover sheet per group; upload with Stage 2 link in Week 12. |

---

## 8) Quick glossary

| Term | Plain meaning |
|:--|:--|
| Encapsulation | Private fields, controlled access via methods. |
| Validation | Check inputs; log and skip invalid values. |
| DRY | “Do not repeat yourself” — centralise shared logic. |
| Defensive coding | Anticipate invalid states; use early‑exit guards. |
| Comparable/Comparator | Define object ordering (natural vs custom). |
| Equality & hashing | `equals`/`hashCode` for correct behaviour in sets/maps. |
| JUnit | Java unit testing framework. |

---

## Appendix A — Complete Assessment Rubric

> **How to read this rubric:** Each criterion shows a weight (% of overall CA1 mark). Performance bands describe expected evidence. Mark within a band should reflect consistency, completeness, and polish. Evidence can appear in code, commits, README/report, demo, tests, and the interview.

### A.1 Stage 1 — Design & Prototype (40%)

| Criterion (Weight) | Excellent (A) | Good (B) | Satisfactory (C) | Limited (D/E) | Unacceptable (F/NS) |
|:--|:--|:--|:--|:--|:--|
| **Entity Definition (6%)** | 7+ well‑typed fields; names and ranges justified; validation clear and consistent; no duplicated logic. | 7+ fields; mostly appropriate; minor naming/range/validation issues. | Minimum met; validation exists but patchy; some unclear names. | Fields/validation inconsistent; weak justification; signs of copy‑paste. | Missing or trivial model; no validation; cannot proceed. |
| **CSV Loading (5%)** | Robust loader; trims/keeps empties; clear logging; prints valid count + sample; handles malformed lines gracefully. | Mostly robust; handles common errors; minor logging/printing weaknesses. | Loads 10 rows; basic error handling; prints output. | Fragile; skips logic unclear; misleading output. | Fails to load or crashes; no usable output. |
| **Data Structure Choice (3%)** | Justifies `ArrayList` vs `LinkedList` with access/mutation pattern evidence; safe iterator removal demonstrated. | Choice reasoned; minor misuse. | Choice present; limited justification; removal safe or partially correct. | Weak/incorrect justification; unsafe removal. | No rationale; incorrect usage; runtime errors. |
| **Ordering & Searching (4%)** | Correct `Comparable`; one clear `Comparator`; consistent with equals where relevant; search/sort demoed and explained. | Correct implementations; small edge cases missed. | Works for common cases; edge cases not discussed. | Partially working; misunderstandings visible. | Not implemented or incorrect (e.g., violates comparator contract). |
| **DRY & Defensive Coding (3%)** | Clear helpers, guard clauses, centralised parsing; repetition minimal. | Some helpers; a little repetition remains. | DRY attempts visible; repetition in places. | Repetition common; guards inconsistent. | No DRY; no guards; frequent ad‑hoc code. |
| **Reporting & Output (3%)** | Concise, readable output; aligns with README; shows sorted/filtered examples. | Mostly clear; occasional verbosity. | Adequate; minimal formatting. | Hard to follow; lacks structure. | Missing or unreadable. |
| **Repository Quality (4%)** | Regular, meaningful commits; tags/branches; clear pair balance. | Good cadence; mostly clear messages. | Adequate commits; uneven contributions. | Sparse or bursty commits; unclear authorship. | Single dump; no evidence of process. |
| **Unit Testing & Coverage ≥35% (4%)** | Clear AAA tests; boundary/negative cases; coverage evidence linked. | Good variety; coverage adequate. | Basic happy‑path tests; minimal evidence. | Few tests; flaky or poorly named. | No tests or non‑running tests. |
| **Reporting & Documentation (8%)** | Focused README/report: domain, design rationale, 4–6 defensive examples, contribution matrix, reflections; within word ranges. | Complete but slightly verbose/short; minor gaps. | Meets minimum content; uneven depth. | Incomplete or off‑topic; misses key items. | Missing or plagiarised. |

**Stage 1 subtotal: 40%**

---

### A.2 Stage 2 — Implementation & Verification (60%)

| Criterion (Weight) | Excellent (A) | Good (B) | Satisfactory (C) | Limited (D/E) | Unacceptable (F/NS) |
|:--|:--|:--|:--|:--|:--|
| **Extended Dataset (5%)** | Efficient 1000‑row load; robust parsing; clear error logs; timings/notes on performance. | Robust with minor inefficiencies. | Loads correctly; basic logging. | Fragile or slow; unclear logging. | Fails to load reliably. |
| **Entity Enhancements (6%)** | Helpful helpers/utilities; invariants maintained; no leaks; defensive copies where needed. | Enhancements mostly solid; small leaks/edge issues. | Adequate additions; some ad‑hoc code. | Weak or inconsistent; side‑effects/leaks present. | No meaningful enhancement or broken invariants. |
| **Collections & Lookup (5%)** | Appropriate use of `HashSet`/`HashMap`; duplicate detection/lookup explained with examples. | Correct choice; minor gaps in examples. | Works for common cases; sparse rationale. | Misused or inefficient structures. | Not implemented or incorrect behaviour. |
| **Equality & Hashing (6%)** | Correct `equals`/`hashCode` over stable identity; demos show effects in sets/maps; contract documented. | Correct for most cases; minor contract omissions. | Works in typical cases; limited demos. | Partially correct; risks violations. | Incorrect or missing; breaks collections. |
| **Advanced Queries & CSV Export (6%)** | 2+ meaningful queries (range/top‑N/frequency) with clear output; CSV export schema‑compatible. | Queries correct; small formatting/export issues. | Queries present; basic export. | Queries simplistic; export inconsistent. | Missing queries/export. |
| **Testing Demonstration (5%)** | JUnit 5 battery for sorting/search/duplicates/equals‑hash; good naming and assertions. | Good coverage of key paths. | Basic positives and some negatives. | Minimal or shallow tests. | None or non‑running. |
| **Defensive & Reusable Code (3%)** | Consistent guard/validation; utilities reused across stages; minimal duplication. | Mostly consistent; a few repetitions. | Acceptable; repetition persists. | Inconsistent; brittle. | Absent. |
| **Repository Quality (6%)** | Balanced contributions; PRs/reviews (if used); clear commit history. | Good overall; minor balance issues. | Adequate; some uneven work. | Poor hygiene or unclear history. | Dumped or opaque history. |
| **Unit Testing & Coverage ≥75% (6%)** | Clear evidence (Jacoco); mix of boundary/negative/edge; mutation or property‑based tests considered. | Strong coverage; mostly meaningful. | Meets target; some trivial tests. | Below target or weak tests. | No evidence or far below target. |
| **Report + Presentation/Interview (12%)** | Professional report; strong demo; **interview** shows deep understanding of design, code, and tests; justifies trade‑offs. | Clear report/demo; good interview answers. | Adequate report; basic demo; interview acceptable. | Weak report/demo; interview uncertain. | Missing report or demo; interview fails to evidence understanding. |

**Stage 2 subtotal: 60%**

> **Oral Defense policy:** Interview evidence may adjust Stage 2 marks for criteria where understanding must be demonstrated. Where the defense reveals a lack of understanding of submitted code, this will be reflected in the **Stage 1 and Stage 2** grade in line with academic integrity and module policy.

**Total for CA1:** 100%

